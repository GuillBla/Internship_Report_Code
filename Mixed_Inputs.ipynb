{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mixed Inputs",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuillBla/Internship_Report_Code/blob/main/Mixed_Inputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A script to execute to prevent from the limits of imposed by Google Colab"
      ],
      "metadata": {
        "id": "xC8OUD0R3sWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function ConnectButton(){\n",
        "#     console.log(\"Connect pushed\"); \n",
        "#     document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "# }\n",
        "# setInterval(ConnectButton,60000);"
      ],
      "metadata": {
        "id": "kmRTtDEi3wQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders\n",
        "!pip install livelossplot"
      ],
      "metadata": {
        "id": "MsFccFen30iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os,shutil\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import csv\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Dense, Dropout, Flatten,Conv2D, GlobalAveragePooling2D\n",
        "from keras import regularizers\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import concatenate\n",
        "import numpy as np\n",
        "import argparse\n",
        "import locale\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "BG-9OY0i32lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u8y8OSZL4pvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the following folders have already been created, they need to be removed (uncomment if it is the case)"
      ],
      "metadata": {
        "id": "486QrigNxJma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Clear the environment\n",
        "# shutil.rmtree(r'/content/topic_classification_splitted')\n",
        "# shutil.rmtree(r'/content/topic_classification')"
      ],
      "metadata": {
        "id": "5gMi5JqP4ue9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile('/content/drive/MyDrive/topic_classification.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/content') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "nMlfx1VC42ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I split the dataset into two folders : \n",
        "1. One containing the training set : /content/topic_classification_splitted/train\n",
        "2. One containing the testing set : /content/topic_classification_splitted/test\n",
        "\n",
        "Note that I will split the first folder in two to create a validation set."
      ],
      "metadata": {
        "id": "AysI4lEQ5N95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitfolders.fixed(r\"/content/topic_classification\", output=r\"/content/topic_classification_splitted\",\n",
        "    seed=2504, fixed=(1, 0, 0), oversample=False, group_prefix=None, move=False)\n",
        "\n",
        "# os.rename(\"/content/topic_classification_splitted/val\", \"/content/topic_classification_splitted/test\")\n",
        "\n",
        "names_classes = [f for f in listdir('/content/topic_classification_splitted/test')]"
      ],
      "metadata": {
        "id": "bcfuQRpVxJPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting HMTL Input "
      ],
      "metadata": {
        "id": "Y-Rw5dSPvUSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The topics of the dataset we are looking for :\n",
        "topics = [\"Accommodation\", \"Administration\", \"Audiovisual\", \"Automotive\", \"Casino\", \"Dating\", \"Drugs\", \"Engineering\",\n",
        "          \"Fashion\", \"Finance\", \"Food\", \"Health\", \"Office\", \"Software\", \"Insurance\", \"Laboratory\", \"Lawyer\",\n",
        "          \"Entertainment\", \"Logistic\", \"Marketing\", \"Personal\",\"Skills\", \"Politic\", \"Porn\", \"Religion\", \"Sustainability\",\n",
        "          \"Technology\"]\n",
        "\n",
        "dictionnary = {\"Accommodation\" :1 , \"Administration\":2, \"Audiovisual\":3, \"Automotive\":4, \"Casino & Gambling\":5, \"Dating\":6, \"Drugs\":7, \"Engineering & Building\":8,\n",
        "          \"Fashion\":9, \"Finance\":10, \"Food & Drinks\":11, \"Health & Wellness\":12, \"Home & Office\":13, \"Hosting & Software\":14, \"Insurance\":15, \"Laboratory\":16, \"Lawyer\":17,\n",
        "          \"Leisure & Entertainment\":18, \"Logistic\":19, \"Marketing\":20, \"Personal\":21, \"Personal Skills\":22, \"Politic\":23, \"Porn\":24, \"Religion\":25, \"Sustainability\":26,\n",
        "          \"Technology & Electronic\":27}\n",
        "\n",
        "path = \"/content/topic_classification_splitted\"\n",
        "\n",
        "blacklist = ['[document]', 'noscript', 'html', 'meta', 'input', 'script']\n",
        "\n",
        "# A function to search for a pattern : key, in a string : content\n",
        "def recherche(key, content, extract=False):\n",
        "    nkey = 0\n",
        "    content = content.lower()\n",
        "    key = key.lower()\n",
        "    for k in range(len(content)):\n",
        "        if content[k:k + len(key)] == key:\n",
        "            nkey += 1\n",
        "    return nkey\n",
        "\n",
        "with open('html_input.csv', 'w', newline='') as file: \n",
        "  writer = csv.writer(file)\n",
        "\n",
        "  for (root, dirs, file) in os.walk(path):\n",
        "    for url in file:\n",
        "      if '.jpg' in url:\n",
        "        # Cleaning the names of the files to urls\n",
        "        url = url.replace('http_', \"http://\")\n",
        "        url = url.replace('https_', \"https://\")\n",
        "        url = url.replace('https_www', \"https://\")\n",
        "        url = url.replace('http_www', \"http://\")\n",
        "        url = url.replace('.jpg', \"\")\n",
        "\n",
        "        # HTML request :\n",
        "        try:\n",
        "          res = requests.get(url)\n",
        "          html_page = res.content\n",
        "          soup = BeautifulSoup(html_page, 'html.parser')\n",
        "          puretext = soup.find_all(text=True)\n",
        "          htmltext = soup.find_all()\n",
        "          htmloutput = ''\n",
        "\n",
        "          for t in htmltext:\n",
        "            if t.parent.name not in blacklist:\n",
        "              htmloutput += '{} '.format(t)\n",
        "       \n",
        "          #retrieve true topic : \n",
        "          category = re.split(\"/\", root, 4)[-1] \n",
        "          row=[dictionnary[category],url]\n",
        "          # Looking for possible topics in HTML code and adding the data to the csv file\n",
        "          for topic in topics:\n",
        "            row.append(recherche(topic, htmloutput))\n",
        "          writer.writerow(row)\n",
        "        except Exception as e:\n",
        "          break"
      ],
      "metadata": {
        "id": "riQfOkjawcKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the .csv file with the HMTL extraction results as a DataFrame"
      ],
      "metadata": {
        "id": "rMdXrHAZwcyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-GhTuLtuvm3"
      },
      "outputs": [],
      "source": [
        "def load_html_attributes(inputPath):\n",
        "\t# initialize the list of column names in the CSV file and then\n",
        "\t# load it using Pandas\n",
        "\tcols = [\"category\",\"url\", \"Accommodation\", \"Administration\", \"Audiovisual\", \"Automotive\", \"Casino & Gambling\", \"Dating\", \"Drugs\", \"Engineering & Building\",\n",
        "          \"Fashion\", \"Finance\", \"Food & Drinks\", \"Health & Wellness\", \"Home & Office\", \"Hosting & Software\", \"Insurance\", \"Laboratory\", \"Lawyer\",\n",
        "          \"Leisure & Entertainment\", \"Logistic\", \"Marketing\", \"Personal\", \"Personal Skills\", \"Politic\", \"Porn\", \"Religion\", \"Sustainability\",\n",
        "          \"Technology & Electronic\"]\n",
        "\tdf = pd.read_csv(inputPath, sep=\",\", header=None, names=cols)\n",
        "\t\n",
        "\treturn df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating data from images"
      ],
      "metadata": {
        "id": "tDzhFdbzTEFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(df, inputPath):\n",
        "\timages = []\n",
        "\tdata_path = inputPath\n",
        "\n",
        "\tfor category in listdir(data_path):\n",
        "\t\tfor image in listdir(data_path+ \"/\" + category):\n",
        "\t\t\turl = image\n",
        "\t\t\turl = url.replace('http_', \"http://\")\n",
        "\t\t\turl = url.replace('https_', \"https://\")\n",
        "\t\t\turl = url.replace('https_www', \"https://\")\n",
        "\t\t\turl = url.replace('http_www', \"http://\")\n",
        "\t\t\turl = url.replace('.jpg', \"\")\n",
        "\t\t\tif (any(df['url'] == url)):\n",
        "\t\t\t\timg = cv2.imread(data_path + \"/\" + category + \"/\" + image , cv2.IMREAD_COLOR)\n",
        "\t\t\t\t# resize makes problems but this solves it :\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\timg_resize = cv2.resize(img,(224,224))\n",
        "\t\t\t\t\timages.append(img_resize)\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\timg_data = np.array(images)\n",
        "\timg_data = img_data.astype('float32')\n",
        "\timg_data /= 255\n",
        "\treturn img_data\n"
      ],
      "metadata": {
        "id": "i2vlSyxDTCAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions two create the two models that we will link"
      ],
      "metadata": {
        "id": "EFp2FgTolMAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Multi-Layer Perceptron\n",
        "def create_mlp(dim):\n",
        " model = Sequential()\n",
        " model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        " model.add(Dense(4, activation=\"relu\"))\n",
        " return model\n",
        "\n",
        "def create_cnn(input_shape, n_classes):\n",
        "  base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "  model = base_model.output\n",
        "  model = GlobalAveragePooling2D()(model)\n",
        "  model = Dense(256, activation=\"relu\", name=\"HiddenLayer1\")(model)\n",
        "  model = Dropout(0.5)(model)\n",
        "  model = Dense(128, activation=\"relu\", name=\"HiddenLayer2\", kernel_regularizer=regularizers.l2(0.01))(model)\n",
        "  model = Dropout(0.2)(model)\n",
        "  model = Dense(27, activation=\"softmax\", name=\"OutputLayer\")(model)\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=model)\n",
        "  return model"
      ],
      "metadata": {
        "id": "m_FVEV-hlP_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df is a Dataframe containing all of the HTML informations\n",
        "print(\"[INFO] loading html attributes...\")\n",
        "inputPath = r\"/content/html_input.csv\"\n",
        "# df = datasets.load_html_attributes(inputPath)\n",
        "df_preprocess = load_html_attributes(inputPath)\n",
        "df_training = df_preprocess\n",
        "#delete \"url\" column\n",
        "df_training = df_training.drop(['url'], axis=1).astype(np.float32)\n",
        "\n",
        "# images is a list of the images of the dataset\n",
        "print(\"[INFO] loading images...\")\n",
        "# images = datasets.load_house_images(df, r\"/content/topic_classification_splitted/test\")\n",
        "images = load_images(df_preprocess, r\"/content/topic_classification_splitted/train\")"
      ],
      "metadata": {
        "id": "bIS1c6Zg1TjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into train and test splits"
      ],
      "metadata": {
        "id": "V5lBs6GFeXof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "print(\"[INFO] processing data...\")\n",
        "split = train_test_split(df_training, images, test_size=0.2, random_state=2504)\n",
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n",
        "\n",
        "trainY = trainAttrX[\"category\"]\n",
        "testY = testAttrX[\"category\"] "
      ],
      "metadata": {
        "id": "RZf7UzWR3IX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the MLP and CNN models\n",
        "mlp = create_mlp(trainAttrX.shape[1])\n",
        "\n",
        "input_shape = (224, 224, 3)\n",
        "n_classes=27\n",
        "cnn = create_cnn(input_shape, n_classes)\n",
        "\n",
        "# create the input to our final set of layers as the *output* of both\n",
        "# the MLP and CNN\n",
        "combinedInput = concatenate([mlp.output, cnn.output])\n",
        "\n",
        "x = Dense(4, activation=\"relu\")(combinedInput)\n",
        "x = Dense(27, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=[mlp.input, cnn.input], outputs=x)"
      ],
      "metadata": {
        "id": "9b-2z30w3M9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the model"
      ],
      "metadata": {
        "id": "6sAzoxzXns3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Plot the model\n",
        "plot_model(model, to_file='model.png')\n",
        "\n",
        "# Display the image\n",
        "data = plt.imread('model.png')\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r35BraQknrCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(\n",
        "\tx=[trainAttrX, trainImagesX], y=trainY,\n",
        "\tvalidation_data=([testAttrX, testImagesX], testY),\n",
        "\tepochs=30, batch_size=8)\n"
      ],
      "metadata": {
        "id": "cMXg4eqh3PUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "print(\"[INFO] Computing metrics...\")\n",
        "preds = model.predict([testAttrX, testImagesX])\n",
        "prediction = []\n",
        "for pred in preds:\n",
        "  max = 0\n",
        "  best = -1\n",
        "  for p in range(len(pred)):\n",
        "    if pred[p]> max:\n",
        "      max = pred[p]\n",
        "      best = p\n",
        "  prediction.append(best)\n",
        "\n",
        "print(\"Predicted classes: \")\n",
        "print(prediction)\n",
        "true_classes = []\n",
        "for i in range(len(prediction)):\n",
        "  true_classes.append(testY.iloc[i])\n",
        "print(\"True classes: \")\n",
        "print(true_classes)\n",
        "\n",
        "diff = np.array(prediction) - np.array(true_classes)\n",
        "sum=0\n",
        "for i in range (len(diff)):\n",
        "  if diff[i] == 0:\n",
        "    sum +=1\n",
        "accuracy = sum/len(diff)\n",
        "print(\"Accuracy : \"+ str(accuracy))"
      ],
      "metadata": {
        "id": "XQca-4PXB5oN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}